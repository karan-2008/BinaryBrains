"""
AI Insight Engine — Phase 3/4.

Generates natural-language advisory insights by calling a local Ollama instance.
This module is the ONLY place where LLM calls are made.

STRICT RULES:
- The AI does NOT perform calculations.
- The AI only explains pre-computed data.
- All numeric values are injected into the prompt, never generated by the model.
"""

import os
import requests
from app.utils.text_parser import sanitize_ai_response
from app.utils.logger import get_logger

logger = get_logger(__name__)

# ---------------------------------------------------------------------------
# Ollama Configuration
# ---------------------------------------------------------------------------
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434/api/generate")
OLLAMA_API_KEY = os.getenv("OLLAMA_API_KEY", "")
MODEL_NAME = os.getenv("OLLAMA_MODEL", "deepseek-v3.1:671b-cloud")


def generate_drought_insight(
    village_name: str,
    population: int,
    wsi: float,
    status: str,
    r_dev: float,
    g_drop: float,
    tankers: int,
    target_language: str = "English",
) -> str:
    """
    Injects deterministic math results into a strict prompt and calls local Ollama.

    Args:
        village_name: Name of the village.
        population: Village population.
        wsi: Pre-computed Water Stress Index (0–100).
        status: Human-readable WSI status label.
        r_dev: Rainfall deviation percentage.
        g_drop: Groundwater drop in meters.
        tankers: Number of tankers allocated.
        target_language: Language for the response (default: "English").

    Returns:
        Cleaned advisory text with exactly 3 bullet points,
        or an error message string if the LLM call fails.
    """
    prompt = f"""You are an expert Hydrologist AI assisting the District Collector.
Analyze the following deterministic village data and provide a strict 3-bullet point action plan.
DO NOT perform any calculations. Use the provided data. Do NOT include any <think> tags or reasoning blocks.

Data Context:
- Village: {village_name}
- Population: {population}
- Water Stress Index (WSI): {wsi}/100 (Status: {status})
- Rainfall Deviation: {r_dev}%
- Groundwater Drop: {g_drop}m
- Tankers Allocated: {tankers}

Output Format MUST be exactly 3 numbered bullet points:
1. Primary Cause: [One sentence explaining why the WSI is at this level based on the data]
2. Impact: [One sentence on population risk]
3. Directive: [One sentence on immediate administrative action]

Output Language: {target_language}
"""

    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False,
    }

    headers = {"Content-Type": "application/json"}
    if OLLAMA_API_KEY:
        headers["Authorization"] = f"Bearer {OLLAMA_API_KEY}"

    logger.info(f"Requesting AI insight for village: {village_name} (lang={target_language}, model={MODEL_NAME})")

    try:
        response = requests.post(OLLAMA_URL, json=payload, headers=headers, timeout=120)
        response.raise_for_status()

        response_data = response.json()
        raw_output = response_data.get("response", "")

        if not raw_output.strip():
            logger.warning(f"Ollama returned empty response for {village_name}")
            return "Error: Ollama returned an empty response. The model may still be loading."

        # Sanitize the output before returning
        clean_output = sanitize_ai_response(raw_output)

        logger.info(f"AI insight generated for {village_name}: {len(clean_output)} chars")
        return clean_output

    except requests.exceptions.Timeout:
        logger.error(f"Ollama timed out for {village_name} (120s)")
        return "Error: AI model timed out. The 671B model may need more time. Please retry."
    except requests.exceptions.ConnectionError:
        logger.error("Cannot connect to Ollama. Is it running?")
        return "Error: Cannot connect to Ollama at " + OLLAMA_URL + ". Ensure Ollama is running."
    except requests.exceptions.RequestException as e:
        logger.error(f"Ollama Request Error: {e}")
        return f"Error: {str(e)}"

def query_ollama(prompt: str, timeout_sec: int = 120) -> str:
    """
    Generic wrapper to query the local Ollama instance.
    """
    try:
        response = requests.post(
            OLLAMA_URL,
            json={
                "model": MODEL_NAME,
                "prompt": prompt,
                "stream": False
            },
            headers={"Authorization": f"Bearer {OLLAMA_API_KEY}"} if OLLAMA_API_KEY else {},
            timeout=timeout_sec
        )
        response.raise_for_status()
        
        raw_text = response.json().get("response", "")
        return sanitize_ai_response(raw_text)
        
    except requests.exceptions.RequestException as e:
        logger.error(f"Ollama API Error: {e}")
        return f"Error: Failed to connect to AI Insight Engine ({e})"
